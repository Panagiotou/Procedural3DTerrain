{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Restart ProGAN (4) (1).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"6C1l-sW7IQIH","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593958143447,"user_tz":-180,"elapsed":896,"user":{"displayName":"Manos Panagiotou","photoUrl":"","userId":"06697745822174955863"}}},"source":["#DataTools.py\n","\"\"\" Module for the data loading pipeline for the model to train \"\"\"\n","\n","\n","def get_transform(new_size=None):\n","    \"\"\"\n","    obtain the image transforms required for the input data\n","    :param new_size: size of the resized images\n","    :return: image_transform => transform object from TorchVision\n","    \"\"\"\n","    from torchvision.transforms import ToTensor, Normalize, Compose, Resize\n","\n","    if new_size is not None:\n","        image_transform = Compose([\n","            Resize(new_size),\n","            ToTensor(),\n","            Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n","        ])\n","\n","    else:\n","        image_transform = Compose([\n","            ToTensor(),\n","            Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n","        ])\n","    return image_transform\n","\n","\n","def get_data_loader(dataset, batch_size, num_workers):\n","    \"\"\"\n","    generate the data_loader from the given dataset\n","    :param dataset: dataset for training (Should be a PyTorch dataset)\n","                    Make sure every item is an Image\n","    :param batch_size: batch size of the data\n","    :param num_workers: num of parallel readers\n","    :return: dl => dataloader for the dataset\n","    \"\"\"\n","    from torch.utils.data import DataLoader\n","\n","    dl = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=num_workers\n","    )\n","\n","    return dl"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"QYp4Pxx0IV5k","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593958148150,"user_tz":-180,"elapsed":5580,"user":{"displayName":"Manos Panagiotou","photoUrl":"","userId":"06697745822174955863"}}},"source":["#Losses.py\n","\"\"\" Module implementing various loss functions \"\"\"\n","\n","import torch as th\n","\n","\n","# =============================================================\n","# Interface for the losses\n","# =============================================================\n","\n","class GANLoss:\n","    \"\"\" Base class for all losses\n","\n","        @args:\n","            dis: Discriminator used for calculating the loss\n","                 Note this must be a part of the GAN framework\n","    \"\"\"\n","\n","    def __init__(self, dis):\n","        self.dis = dis\n","\n","    def dis_loss(self, real_samps, fake_samps, height, alpha):\n","        \"\"\"\n","        calculate the discriminator loss using the following data\n","        :param real_samps: batch of real samples\n","        :param fake_samps: batch of generated (fake) samples\n","        :param height: current height at which training is going on\n","        :param alpha: current value of the fader alpha\n","        :return: loss => calculated loss Tensor\n","        \"\"\"\n","        raise NotImplementedError(\"dis_loss method has not been implemented\")\n","\n","    def gen_loss(self, real_samps, fake_samps, height, alpha):\n","        \"\"\"\n","        calculate the generator loss\n","        :param real_samps: batch of real samples\n","        :param fake_samps: batch of generated (fake) samples\n","        :param height: current height at which training is going on\n","        :param alpha: current value of the fader alpha\n","        :return: loss => calculated loss Tensor\n","        \"\"\"\n","        raise NotImplementedError(\"gen_loss method has not been implemented\")\n","\n","\n","class ConditionalGANLoss:\n","    \"\"\" Base class for all conditional losses \"\"\"\n","\n","    def __init__(self, dis):\n","        self.dis = dis\n","\n","    def dis_loss(self, real_samps, fake_samps, labels, height, alpha):\n","        raise NotImplementedError(\"dis_loss method has not been implemented\")\n","\n","    def gen_loss(self, real_samps, fake_samps, labels, height, alpha):\n","        raise NotImplementedError(\"gen_loss method has not been implemented\")\n","\n","\n","# =============================================================\n","# Normal versions of the Losses:\n","# =============================================================\n","\n","class StandardGAN(GANLoss):\n","\n","    def __init__(self, dis):\n","        from torch.nn import BCEWithLogitsLoss\n","\n","        super().__init__(dis)\n","\n","        # define the criterion and activation used for object\n","        self.criterion = BCEWithLogitsLoss()\n","\n","    def dis_loss(self, real_samps, fake_samps, height, alpha):\n","        # small assertion:\n","        assert real_samps.device == fake_samps.device, \\\n","            \"Real and Fake samples are not on the same device\"\n","\n","        # device for computations:\n","        device = fake_samps.device\n","\n","        # predictions for real images and fake images separately :\n","        r_preds = self.dis(real_samps, height, alpha)\n","        f_preds = self.dis(fake_samps, height, alpha)\n","\n","        # calculate the real loss:\n","        real_loss = self.criterion(\n","            th.squeeze(r_preds),\n","            th.ones(real_samps.shape[0]).to(device))\n","\n","        # calculate the fake loss:\n","        fake_loss = self.criterion(\n","            th.squeeze(f_preds),\n","            th.zeros(fake_samps.shape[0]).to(device))\n","\n","        # return final losses\n","        return (real_loss + fake_loss) / 2\n","\n","    def gen_loss(self, _, fake_samps, height, alpha):\n","        preds, _, _ = self.dis(fake_samps, height, alpha)\n","        return self.criterion(th.squeeze(preds),\n","                              th.ones(fake_samps.shape[0]).to(fake_samps.device))\n","\n","\n","class WGAN_GP(GANLoss):\n","\n","    def __init__(self, dis, drift=0.001, use_gp=False):\n","        super().__init__(dis)\n","        self.drift = drift\n","        self.use_gp = use_gp\n","\n","    def __gradient_penalty(self, real_samps, fake_samps,\n","                           height, alpha, reg_lambda=10):\n","        \"\"\"\n","        private helper for calculating the gradient penalty\n","        :param real_samps: real samples\n","        :param fake_samps: fake samples\n","        :param height: current depth in the optimization\n","        :param alpha: current alpha for fade-in\n","        :param reg_lambda: regularisation lambda\n","        :return: tensor (gradient penalty)\n","        \"\"\"\n","        batch_size = real_samps.shape[0]\n","\n","        # generate random epsilon\n","        epsilon = th.rand((batch_size, 1, 1, 1)).to(fake_samps.device)\n","\n","        # create the merge of both real and fake samples\n","        merged = epsilon * real_samps + ((1 - epsilon) * fake_samps)\n","        merged.requires_grad_(True)\n","\n","        # forward pass\n","        op = self.dis(merged, height, alpha)\n","\n","        # perform backward pass from op to merged for obtaining the gradients\n","        gradient = th.autograd.grad(outputs=op, inputs=merged,\n","                                    grad_outputs=th.ones_like(op), create_graph=True,\n","                                    retain_graph=True, only_inputs=True)[0]\n","\n","        # calculate the penalty using these gradients\n","        gradient = gradient.view(gradient.shape[0], -1)\n","        penalty = reg_lambda * ((gradient.norm(p=2, dim=1) - 1) ** 2).mean()\n","\n","        # return the calculated penalty:\n","        return penalty\n","\n","    def dis_loss(self, real_samps, fake_samps, height, alpha):\n","        # define the (Wasserstein) loss\n","        fake_out = self.dis(fake_samps, height, alpha)\n","        real_out = self.dis(real_samps, height, alpha)\n","\n","        loss = (th.mean(fake_out) - th.mean(real_out)\n","                + (self.drift * th.mean(real_out ** 2)))\n","\n","        if self.use_gp:\n","            # calculate the WGAN-GP (gradient penalty)\n","            gp = self.__gradient_penalty(real_samps, fake_samps, height, alpha)\n","            loss += gp\n","\n","        return loss\n","\n","    def gen_loss(self, _, fake_samps, height, alpha):\n","        # calculate the WGAN loss for generator\n","        loss = -th.mean(self.dis(fake_samps, height, alpha))\n","\n","        return loss\n","\n","\n","class LSGAN(GANLoss):\n","\n","    def __init__(self, dis):\n","        super().__init__(dis)\n","\n","    def dis_loss(self, real_samps, fake_samps, height, alpha):\n","        return 0.5 * (th.mean((self.dis(real_samps, height, alpha) - 1) ** 2)\n","                      + (th.mean(self.dis(fake_samps, height, alpha) ** 2)))\n","\n","    def gen_loss(self, _, fake_samps, height, alpha):\n","        return 0.5 * (th.mean((self.dis(fake_samps, height, alpha) - 1) ** 2))\n","\n","\n","class LSGAN_SIGMOID(GANLoss):\n","\n","    def __init__(self, dis):\n","        super().__init__(dis)\n","\n","    def dis_loss(self, real_samps, fake_samps, height, alpha):\n","        from torch.nn.functional import sigmoid\n","        real_scores = sigmoid(self.dis(real_samps, height, alpha))\n","        fake_scores = sigmoid(self.dis(fake_samps, height, alpha))\n","        return 0.5 * ((th.mean((real_scores - 1) ** 2)) + th.mean(fake_scores ** 2))\n","\n","    def gen_loss(self, _, fake_samps, height, alpha):\n","        from torch.nn.functional import sigmoid\n","        scores = sigmoid(self.dis(fake_samps, height, alpha))\n","        return 0.5 * (th.mean((scores - 1) ** 2))\n","\n","\n","class HingeGAN(GANLoss):\n","\n","    def __init__(self, dis):\n","        super().__init__(dis)\n","\n","    def dis_loss(self, real_samps, fake_samps, height, alpha):\n","        r_preds = self.dis(real_samps, height, alpha)\n","        f_preds = self.dis(fake_samps, height, alpha)\n","\n","        loss = (th.mean(th.nn.ReLU()(1 - r_preds)) +\n","                th.mean(th.nn.ReLU()(1 + f_preds)))\n","\n","        return loss\n","\n","    def gen_loss(self, _, fake_samps, height, alpha):\n","        return -th.mean(self.dis(fake_samps, height, alpha))\n","\n","\n","class RelativisticAverageHingeGAN(GANLoss):\n","\n","    def __init__(self, dis):\n","        super().__init__(dis)\n","\n","    def dis_loss(self, real_samps, fake_samps, height, alpha):\n","        # Obtain predictions\n","        r_preds = self.dis(real_samps, height, alpha)\n","        f_preds = self.dis(fake_samps, height, alpha)\n","\n","        # difference between real and fake:\n","        r_f_diff = r_preds - th.mean(f_preds)\n","\n","        # difference between fake and real samples\n","        f_r_diff = f_preds - th.mean(r_preds)\n","\n","        # return the loss\n","        loss = (th.mean(th.nn.ReLU()(1 - r_f_diff))\n","                + th.mean(th.nn.ReLU()(1 + f_r_diff)))\n","\n","        return loss\n","\n","    def gen_loss(self, real_samps, fake_samps, height, alpha):\n","        # Obtain predictions\n","        r_preds = self.dis(real_samps, height, alpha)\n","        f_preds = self.dis(fake_samps, height, alpha)\n","\n","        # difference between real and fake:\n","        r_f_diff = r_preds - th.mean(f_preds)\n","\n","        # difference between fake and real samples\n","        f_r_diff = f_preds - th.mean(r_preds)\n","\n","        # return the loss\n","        return (th.mean(th.nn.ReLU()(1 + r_f_diff))\n","                + th.mean(th.nn.ReLU()(1 - f_r_diff)))\n","\n","\n","# =============================================================\n","# Conditional versions of the Losses:\n","# =============================================================\n","\n","class CondStandardGAN(ConditionalGANLoss):\n","\n","    def __init__(self, dis):\n","        from torch.nn import BCEWithLogitsLoss\n","\n","        super().__init__(dis)\n","\n","        # define the criterion and activation used for object\n","        self.criterion = BCEWithLogitsLoss()\n","\n","    def dis_loss(self, real_samps, fake_samps, labels, height, alpha):\n","        # small assertion:\n","        assert real_samps.device == fake_samps.device, \\\n","            \"Real and Fake samples are not on the same device\"\n","\n","        # device for computations:\n","        device = fake_samps.device\n","\n","        # predictions for real images and fake images separately:\n","        r_preds = self.dis(real_samps, labels, height, alpha)\n","        f_preds = self.dis(fake_samps, labels, height, alpha)\n","\n","        # calculate the real loss:\n","        real_loss = self.criterion(\n","            th.squeeze(r_preds),\n","            th.ones(real_samps.shape[0]).to(device))\n","\n","        # calculate the fake loss:\n","        fake_loss = self.criterion(\n","            th.squeeze(f_preds),\n","            th.zeros(fake_samps.shape[0]).to(device))\n","\n","        # return final loss\n","        return (real_loss + fake_loss) / 2\n","\n","    def gen_loss(self, _, fake_samps, labels, height, alpha):\n","        preds, _, _ = self.dis(fake_samps, labels, height, alpha)\n","        return self.criterion(th.squeeze(preds),\n","                              th.ones(fake_samps.shape[0]).to(fake_samps.device))\n","\n","\n","class CondWGAN_GP(ConditionalGANLoss):\n","\n","    def __init__(self, dis, drift=0.001, use_gp=False):\n","        super().__init__(dis)\n","        self.drift = drift\n","        self.use_gp = use_gp\n","\n","    def __gradient_penalty(self, real_samps, fake_samps, labels,\n","                           height, alpha, reg_lambda=10):\n","        \"\"\"\n","        private helper for calculating the gradient penalty\n","        :param real_samps: real samples\n","        :param fake_samps: fake samples\n","        :param labels: used for conditional loss calculation\n","                       Note that this is just [Batch x 1] plain integer labels\n","        :param height: current depth in the optimization\n","        :param alpha: current alpha for fade-in\n","        :param reg_lambda: regularisation lambda\n","        :return: tensor (gradient penalty)\n","        \"\"\"\n","        from torch.autograd import grad\n","\n","        batch_size = real_samps.shape[0]\n","\n","        # generate random epsilon\n","        epsilon = th.rand((batch_size, 1, 1, 1)).to(fake_samps.device)\n","\n","        # create the merge of both real and fake samples\n","        merged = (epsilon * real_samps) + ((1 - epsilon) * fake_samps)\n","        merged.requires_grad_(True)\n","\n","        # forward pass\n","        op = self.dis(merged, labels, height, alpha)\n","\n","        # obtain gradient of op wrt. merged\n","        gradient = grad(outputs=op, inputs=merged,\n","                        grad_outputs=th.ones_like(op), create_graph=True,\n","                        retain_graph=True, only_inputs=True)[0]\n","\n","        # calculate the penalty using these gradients\n","        gradient = gradient.view(batch_size, -1)\n","        penalty = reg_lambda * ((gradient.norm(p=2, dim=1) - 1) ** 2).mean()\n","\n","        # return the calculated penalty:\n","        return penalty\n","\n","    def dis_loss(self, real_samps, fake_samps, labels, height, alpha):\n","        # define the (Wasserstein) loss\n","        fake_out = self.dis(fake_samps, labels, height, alpha)\n","        real_out = self.dis(real_samps, labels, height, alpha)\n","\n","        loss = (th.mean(fake_out) - th.mean(real_out)\n","                + (self.drift * th.mean(real_out ** 2)))\n","\n","        if self.use_gp:\n","            # calculate the WGAN-GP (gradient penalty)\n","            gp = self.__gradient_penalty(real_samps, fake_samps,\n","                                         labels, height, alpha)\n","            loss += gp\n","\n","        return loss\n","\n","    def gen_loss(self, _, fake_samps, labels, height, alpha):\n","        # calculate the WGAN loss for generator\n","        loss = -th.mean(self.dis(fake_samps, labels, height, alpha))\n","\n","        return loss\n","\n","\n","class CondLSGAN(ConditionalGANLoss):\n","\n","    def __init__(self, dis):\n","        super().__init__(dis)\n","\n","    def dis_loss(self, real_samps, fake_samps, labels, height, alpha):\n","        return 0.5 * ((th.mean((self.dis(real_samps, labels, height, alpha) - 1) ** 2))\n","                      + (th.mean(self.dis(fake_samps, labels, height, alpha) ** 2)))\n","\n","    def gen_loss(self, _, fake_samps, labels, height, alpha):\n","        return 0.5 * (th.mean((self.dis(fake_samps, labels, height, alpha) - 1) ** 2))\n","\n","\n","class CondLSGAN_SIGMOID(ConditionalGANLoss):\n","\n","    def __init__(self, dis):\n","        super().__init__(dis)\n","\n","    def dis_loss(self, real_samps, fake_samps, labels, height, alpha):\n","        from torch.nn.functional import sigmoid\n","        real_scores = sigmoid(self.dis(real_samps, labels, height, alpha))\n","        fake_scores = sigmoid(self.dis(fake_samps, labels, height, alpha))\n","        return 0.5 * (th.mean((real_scores - 1) ** 2) + th.mean(fake_scores ** 2))\n","\n","    def gen_loss(self, _, fake_samps, labels, height, alpha):\n","        from torch.nn.functional import sigmoid\n","        scores = sigmoid(self.dis(fake_samps, labels, height, alpha))\n","        return 0.5 * (th.mean((scores - 1) ** 2))\n","\n","\n","class CondHingeGAN(ConditionalGANLoss):\n","\n","    def __init__(self, dis):\n","        super().__init__(dis)\n","\n","    def dis_loss(self, real_samps, fake_samps, labels, height, alpha):\n","        r_preds = self.dis(real_samps, labels, height, alpha)\n","        f_preds = self.dis(fake_samps, labels, height, alpha)\n","\n","        loss = (th.mean(th.nn.ReLU()(1 - r_preds)) +\n","                th.mean(th.nn.ReLU()(1 + f_preds)))\n","\n","        return loss\n","\n","    def gen_loss(self, _, fake_samps, labels, height, alpha):\n","        return -th.mean(self.dis(fake_samps, labels, height, alpha))\n","\n","\n","class CondRelativisticAverageHingeGAN(ConditionalGANLoss):\n","\n","    def __init__(self, dis):\n","        super().__init__(dis)\n","\n","    def dis_loss(self, real_samps, fake_samps, labels, height, alpha):\n","        # Obtain predictions\n","        r_preds = self.dis(real_samps, labels, height, alpha)\n","        f_preds = self.dis(fake_samps, labels, height, alpha)\n","\n","        # difference between real and fake:\n","        r_f_diff = r_preds - th.mean(f_preds)\n","\n","        # difference between fake and real samples\n","        f_r_diff = f_preds - th.mean(r_preds)\n","\n","        # return the loss\n","        loss = (th.mean(th.nn.ReLU()(1 - r_f_diff))\n","                + th.mean(th.nn.ReLU()(1 + f_r_diff)))\n","\n","        return loss\n","\n","    def gen_loss(self, real_samps, fake_samps, labels, height, alpha):\n","        # Obtain predictions\n","        r_preds = self.dis(real_samps, labels, height, alpha)\n","        f_preds = self.dis(fake_samps, labels, height, alpha)\n","\n","        # difference between real and fake:\n","        r_f_diff = r_preds - th.mean(f_preds)\n","\n","        # difference between fake and real samples\n","        f_r_diff = f_preds - th.mean(r_preds)\n","\n","        # return the loss\n","        return (th.mean(th.nn.ReLU()(1 + r_f_diff))\n","                + th.mean(th.nn.ReLU()(1 - f_r_diff)))"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wfv0NxHKHp9P","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593958149436,"user_tz":-180,"elapsed":6856,"user":{"displayName":"Manos Panagiotou","photoUrl":"","userId":"06697745822174955863"}}},"source":["#CustomLayers.py\n","\"\"\" Module containing custom layers \"\"\"\n","\n","import torch as th\n","\n","\n","# extending Conv2D and Deconv2D layers for equalized learning rate logic\n","class _equalized_conv2d(th.nn.Module):\n","    \"\"\" conv2d with the concept of equalized learning rate\n","        Args:\n","            :param c_in: input channels\n","            :param c_out:  output channels\n","            :param k_size: kernel size (h, w) should be a tuple or a single integer\n","            :param stride: stride for conv\n","            :param pad: padding\n","            :param bias: whether to use bias or not\n","    \"\"\"\n","\n","    def __init__(self, c_in, c_out, k_size, stride=1, pad=0, bias=True):\n","        \"\"\" constructor for the class \"\"\"\n","        from torch.nn.modules.utils import _pair\n","        from numpy import sqrt, prod\n","\n","        super(_equalized_conv2d, self).__init__()\n","\n","        # define the weight and bias if to be used\n","        self.weight = th.nn.Parameter(th.nn.init.normal_(\n","            th.empty(c_out, c_in, *_pair(k_size))\n","        ))\n","\n","        self.use_bias = bias\n","        self.stride = stride\n","        self.pad = pad\n","\n","        if self.use_bias:\n","            self.bias = th.nn.Parameter(th.FloatTensor(c_out).fill_(0))\n","\n","        fan_in = prod(_pair(k_size)) * c_in  # value of fan_in\n","        self.scale = sqrt(2) / sqrt(fan_in)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        forward pass of the network\n","        :param x: input\n","        :return: y => output\n","        \"\"\"\n","        from torch.nn.functional import conv2d\n","\n","        return conv2d(input=x,\n","                      weight=self.weight * self.scale,  # scale the weight on runtime\n","                      bias=self.bias if self.use_bias else None,\n","                      stride=self.stride,\n","                      padding=self.pad)\n","\n","    def extra_repr(self):\n","        return \", \".join(map(str, self.weight.shape))\n","\n","\n","class _equalized_deconv2d(th.nn.Module):\n","    \"\"\" Transpose convolution using the equalized learning rate\n","        Args:\n","            :param c_in: input channels\n","            :param c_out: output channels\n","            :param k_size: kernel size\n","            :param stride: stride for convolution transpose\n","            :param pad: padding\n","            :param bias: whether to use bias or not\n","    \"\"\"\n","\n","    def __init__(self, c_in, c_out, k_size, stride=1, pad=0, bias=True):\n","        \"\"\" constructor for the class \"\"\"\n","        from torch.nn.modules.utils import _pair\n","        from numpy import sqrt\n","\n","        super(_equalized_deconv2d, self).__init__()\n","\n","        # define the weight and bias if to be used\n","        self.weight = th.nn.Parameter(th.nn.init.normal_(\n","            th.empty(c_in, c_out, *_pair(k_size))\n","        ))\n","\n","        self.use_bias = bias\n","        self.stride = stride\n","        self.pad = pad\n","\n","        if self.use_bias:\n","            self.bias = th.nn.Parameter(th.FloatTensor(c_out).fill_(0))\n","\n","        fan_in = c_in  # value of fan_in for deconv\n","        self.scale = sqrt(2) / sqrt(fan_in)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        forward pass of the layer\n","        :param x: input\n","        :return: y => output\n","        \"\"\"\n","        from torch.nn.functional import conv_transpose2d\n","\n","        return conv_transpose2d(input=x,\n","                                weight=self.weight * self.scale,  # scale the weight on runtime\n","                                bias=self.bias if self.use_bias else None,\n","                                stride=self.stride,\n","                                padding=self.pad)\n","\n","    def extra_repr(self):\n","        return \", \".join(map(str, self.weight.shape))\n","\n","\n","class _equalized_linear(th.nn.Module):\n","    \"\"\" Linear layer using equalized learning rate\n","        Args:\n","            :param c_in: number of input channels\n","            :param c_out: number of output channels\n","            :param bias: whether to use bias with the linear layer\n","    \"\"\"\n","\n","    def __init__(self, c_in, c_out, bias=True):\n","        \"\"\"\n","        Linear layer modified for equalized learning rate\n","        \"\"\"\n","        from numpy import sqrt\n","\n","        super(_equalized_linear, self).__init__()\n","\n","        self.weight = th.nn.Parameter(th.nn.init.normal_(\n","            th.empty(c_out, c_in)\n","        ))\n","\n","        self.use_bias = bias\n","\n","        if self.use_bias:\n","            self.bias = th.nn.Parameter(th.FloatTensor(c_out).fill_(0))\n","\n","        fan_in = c_in\n","        self.scale = sqrt(2) / sqrt(fan_in)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        forward pass of the layer\n","        :param x: input\n","        :return: y => output\n","        \"\"\"\n","        from torch.nn.functional import linear\n","        return linear(x, self.weight * self.scale,\n","                      self.bias if self.use_bias else None)\n","\n","\n","# ----------------------------------------------------------------------------\n","# Pixelwise feature vector normalization.\n","# reference: https://github.com/tkarras/progressive_growing_of_gans/blob/master/networks.py#L120\n","# ----------------------------------------------------------------------------\n","class PixelwiseNorm(th.nn.Module):\n","    def __init__(self):\n","        super(PixelwiseNorm, self).__init__()\n","\n","    def forward(self, x, alpha=1e-8):\n","        \"\"\"\n","        forward pass of the module\n","        :param x: input activations volume\n","        :param alpha: small number for numerical stability\n","        :return: y => pixel normalized activations\n","        \"\"\"\n","        y = x.pow(2.).mean(dim=1, keepdim=True).add(alpha).sqrt()  # [N1HW]\n","        y = x / y  # normalize the input x volume\n","        return y\n","\n","\n","# ==========================================================\n","# Layers required for Building The generator and\n","# discriminator\n","# ==========================================================\n","class GenInitialBlock(th.nn.Module):\n","    \"\"\" Module implementing the initial block of the input \"\"\"\n","\n","    def __init__(self, in_channels, use_eql):\n","        \"\"\"\n","        constructor for the inner class\n","        :param in_channels: number of input channels to the block\n","        :param use_eql: whether to use equalized learning rate\n","        \"\"\"\n","        from torch.nn import LeakyReLU\n","\n","        super(GenInitialBlock, self).__init__()\n","\n","        if use_eql:\n","            self.conv_1 = _equalized_deconv2d(in_channels, in_channels, (4, 4), bias=True)\n","            self.conv_2 = _equalized_conv2d(in_channels, in_channels, (3, 3),\n","                                            pad=1, bias=True)\n","\n","        else:\n","            from torch.nn import Conv2d, ConvTranspose2d\n","            self.conv_1 = ConvTranspose2d(in_channels, in_channels, (4, 4), bias=True)\n","            self.conv_2 = Conv2d(in_channels, in_channels, (3, 3), padding=1, bias=True)\n","\n","        # Pixelwise feature vector normalization operation\n","        self.pixNorm = PixelwiseNorm()\n","\n","        # leaky_relu:\n","        self.lrelu = LeakyReLU(0.2)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        forward pass of the block\n","        :param x: input to the module\n","        :return: y => output\n","        \"\"\"\n","        # convert the tensor shape:\n","        y = th.unsqueeze(th.unsqueeze(x, -1), -1)\n","\n","        # perform the forward computations:\n","        y = self.lrelu(self.conv_1(y))\n","        y = self.lrelu(self.conv_2(y))\n","\n","        # apply pixel norm\n","        y = self.pixNorm(y)\n","\n","        return y\n","\n","\n","class GenGeneralConvBlock(th.nn.Module):\n","    \"\"\" Module implementing a general convolutional block \"\"\"\n","\n","    def __init__(self, in_channels, out_channels, use_eql):\n","        \"\"\"\n","        constructor for the class\n","        :param in_channels: number of input channels to the block\n","        :param out_channels: number of output channels required\n","        :param use_eql: whether to use equalized learning rate\n","        \"\"\"\n","        from torch.nn import LeakyReLU\n","        from torch.nn.functional import interpolate\n","\n","        super(GenGeneralConvBlock, self).__init__()\n","\n","        self.upsample = lambda x: interpolate(x, scale_factor=2)\n","\n","        if use_eql:\n","            self.conv_1 = _equalized_conv2d(in_channels, out_channels, (3, 3),\n","                                            pad=1, bias=True)\n","            self.conv_2 = _equalized_conv2d(out_channels, out_channels, (3, 3),\n","                                            pad=1, bias=True)\n","        else:\n","            from torch.nn import Conv2d\n","            self.conv_1 = Conv2d(in_channels, out_channels, (3, 3),\n","                                 padding=1, bias=True)\n","            self.conv_2 = Conv2d(out_channels, out_channels, (3, 3),\n","                                 padding=1, bias=True)\n","\n","        # Pixelwise feature vector normalization operation\n","        self.pixNorm = PixelwiseNorm()\n","\n","        # leaky_relu:\n","        self.lrelu = LeakyReLU(0.2)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        forward pass of the block\n","        :param x: input\n","        :return: y => output\n","        \"\"\"\n","        y = self.upsample(x)\n","        y = self.pixNorm(self.lrelu(self.conv_1(y)))\n","        y = self.pixNorm(self.lrelu(self.conv_2(y)))\n","\n","        return y\n","\n","\n","# function to calculate the Exponential moving averages for the Generator weights\n","# This function updates the exponential average weights based on the current training\n","def update_average(model_tgt, model_src, beta):\n","    \"\"\"\n","    update the model_target using exponential moving averages\n","    :param model_tgt: target model\n","    :param model_src: source model\n","    :param beta: value of decay beta\n","    :return: None (updates the target model)\n","    \"\"\"\n","\n","    # utility function for toggling the gradient requirements of the models\n","    def toggle_grad(model, requires_grad):\n","        for p in model.parameters():\n","            p.requires_grad_(requires_grad)\n","\n","    # turn off gradient calculation\n","    toggle_grad(model_tgt, False)\n","    toggle_grad(model_src, False)\n","\n","    param_dict_src = dict(model_src.named_parameters())\n","\n","    for p_name, p_tgt in model_tgt.named_parameters():\n","        p_src = param_dict_src[p_name]\n","        assert (p_src is not p_tgt)\n","        p_tgt.copy_(beta * p_tgt + (1. - beta) * p_src)\n","\n","    # turn back on the gradient calculation\n","    toggle_grad(model_tgt, True)\n","    toggle_grad(model_src, True)\n","\n","\n","class MinibatchStdDev(th.nn.Module):\n","    \"\"\"\n","    Minibatch standard deviation layer for the discriminator\n","    \"\"\"\n","\n","    def __init__(self):\n","        \"\"\"\n","        derived class constructor\n","        \"\"\"\n","        super(MinibatchStdDev, self).__init__()\n","\n","    def forward(self, x, alpha=1e-8):\n","        \"\"\"\n","        forward pass of the layer\n","        :param x: input activation volume\n","        :param alpha: small number for numerical stability\n","        :return: y => x appended with standard deviation constant map\n","        \"\"\"\n","        batch_size, _, height, width = x.shape\n","\n","        # [B x C x H x W] Subtract mean over batch.\n","        y = x - x.mean(dim=0, keepdim=True)\n","\n","        # [1 x C x H x W]  Calc standard deviation over batch\n","        y = th.sqrt(y.pow(2.).mean(dim=0, keepdim=False) + alpha)\n","\n","        # [1]  Take average over feature_maps and pixels.\n","        y = y.mean().view(1, 1, 1, 1)\n","\n","        # [B x 1 x H x W]  Replicate over group and pixels.\n","        y = y.repeat(batch_size, 1, height, width)\n","\n","        # [B x C x H x W]  Append as new feature_map.\n","        y = th.cat([x, y], 1)\n","\n","        # return the computed values:\n","        return y\n","\n","\n","class DisFinalBlock(th.nn.Module):\n","    \"\"\" Final block for the Discriminator \"\"\"\n","\n","    def __init__(self, in_channels, use_eql):\n","        \"\"\"\n","        constructor of the class\n","        :param in_channels: number of input channels\n","        :param use_eql: whether to use equalized learning rate\n","        \"\"\"\n","        from torch.nn import LeakyReLU\n","\n","        super(DisFinalBlock, self).__init__()\n","\n","        # declare the required modules for forward pass\n","        self.batch_discriminator = MinibatchStdDev()\n","        if use_eql:\n","            self.conv_1 = _equalized_conv2d(in_channels + 1, in_channels, (3, 3), pad=1, bias=True)\n","            self.conv_2 = _equalized_conv2d(in_channels, in_channels, (4, 4), bias=True)\n","            # final conv layer emulates a fully connected layer\n","            self.conv_3 = _equalized_conv2d(in_channels, 1, (1, 1), bias=True)\n","        else:\n","            from torch.nn import Conv2d\n","            self.conv_1 = Conv2d(in_channels + 1, in_channels, (3, 3), padding=1, bias=True)\n","            self.conv_2 = Conv2d(in_channels, in_channels, (4, 4), bias=True)\n","            # final conv layer emulates a fully connected layer\n","            self.conv_3 = Conv2d(in_channels, 1, (1, 1), bias=True)\n","\n","        # leaky_relu:\n","        self.lrelu = LeakyReLU(0.2)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        forward pass of the FinalBlock\n","        :param x: input\n","        :return: y => output\n","        \"\"\"\n","        # minibatch_std_dev layer\n","        y = self.batch_discriminator(x)\n","\n","        # define the computations\n","        y = self.lrelu(self.conv_1(y))\n","        y = self.lrelu(self.conv_2(y))\n","\n","        # fully connected layer\n","        y = self.conv_3(y)  # This layer has linear activation\n","\n","        # flatten the output raw discriminator scores\n","        return y.view(-1)\n","\n","\n","class ConDisFinalBlock(th.nn.Module):\n","    \"\"\" Final block for the Conditional Discriminator\n","        Uses the Projection mechanism from the paper -> https://arxiv.org/pdf/1802.05637.pdf\n","    \"\"\"\n","\n","    def __init__(self, in_channels, num_classes, use_eql):\n","        \"\"\"\n","        constructor of the class\n","        :param in_channels: number of input channels\n","        :param num_classes: number of classes for conditional discrimination\n","        :param use_eql: whether to use equalized learning rate\n","        \"\"\"\n","        from torch.nn import LeakyReLU, Embedding\n","\n","        super(ConDisFinalBlock, self).__init__()\n","\n","        # declare the required modules for forward pass\n","        self.batch_discriminator = MinibatchStdDev()\n","        if use_eql:\n","            self.conv_1 = _equalized_conv2d(in_channels + 1, in_channels, (3, 3), pad=1, bias=True)\n","            self.conv_2 = _equalized_conv2d(in_channels, in_channels, (4, 4), bias=True)\n","\n","            # final conv layer emulates a fully connected layer\n","            self.conv_3 = _equalized_conv2d(in_channels, 1, (1, 1), bias=True)\n","        else:\n","            from torch.nn import Conv2d\n","            self.conv_1 = Conv2d(in_channels + 1, in_channels, (3, 3), padding=1, bias=True)\n","            self.conv_2 = Conv2d(in_channels, in_channels, (4, 4), bias=True)\n","\n","            # final conv layer emulates a fully connected layer\n","            self.conv_3 = Conv2d(in_channels, 1, (1, 1), bias=True)\n","\n","        # we also need an embedding matrix for the label vectors\n","        self.label_embedder = Embedding(num_classes, in_channels, max_norm=1)\n","\n","        # leaky_relu:\n","        self.lrelu = LeakyReLU(0.2)\n","\n","    def forward(self, x, labels):\n","        \"\"\"\n","        forward pass of the FinalBlock\n","        :param x: input\n","        :param labels: samples' labels for conditional discrimination\n","                       Note that these are pure integer labels [Batch_size x 1]\n","        :return: y => output\n","        \"\"\"\n","        # minibatch_std_dev layer\n","        y = self.batch_discriminator(x)  # [B x C x 4 x 4]\n","\n","        # perform the forward pass\n","        y = self.lrelu(self.conv_1(y))  # [B x C x 4 x 4]\n","\n","        # obtain the computed features\n","        y = self.lrelu(self.conv_2(y))  # [B x C x 1 x 1]\n","\n","        # embed the labels\n","        labels = self.label_embedder(labels)  # [B x C]\n","\n","        # compute the inner product with the label embeddings\n","        y_ = th.squeeze(th.squeeze(y, dim=-1), dim=-1)  # [B x C]\n","        projection_scores = (y_ * labels).sum(dim=-1)  # [B]\n","\n","        # normal discrimination score\n","        y = self.lrelu(self.conv_3(y))  # This layer has linear activation\n","\n","        # calculate the total score\n","        final_score = y.view(-1) + projection_scores\n","\n","        # return the output raw discriminator scores\n","        return final_score\n","\n","\n","class DisGeneralConvBlock(th.nn.Module):\n","    \"\"\" General block in the discriminator  \"\"\"\n","\n","    def __init__(self, in_channels, out_channels, use_eql):\n","        \"\"\"\n","        constructor of the class\n","        :param in_channels: number of input channels\n","        :param out_channels: number of output channels\n","        :param use_eql: whether to use equalized learning rate\n","        \"\"\"\n","        from torch.nn import AvgPool2d, LeakyReLU\n","\n","        super(DisGeneralConvBlock, self).__init__()\n","\n","        if use_eql:\n","            self.conv_1 = _equalized_conv2d(in_channels, in_channels, (3, 3), pad=1, bias=True)\n","            self.conv_2 = _equalized_conv2d(in_channels, out_channels, (3, 3), pad=1, bias=True)\n","        else:\n","            from torch.nn import Conv2d\n","            self.conv_1 = Conv2d(in_channels, in_channels, (3, 3), padding=1, bias=True)\n","            self.conv_2 = Conv2d(in_channels, out_channels, (3, 3), padding=1, bias=True)\n","\n","        self.downSampler = AvgPool2d(2)\n","\n","        # leaky_relu:\n","        self.lrelu = LeakyReLU(0.2)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        forward pass of the module\n","        :param x: input\n","        :return: y => output\n","        \"\"\"\n","        # define the computations\n","        y = self.lrelu(self.conv_1(x))\n","        y = self.lrelu(self.conv_2(y))\n","        y = self.downSampler(y)\n","\n","        return y"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bqld9yfzHSh-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593958151751,"user_tz":-180,"elapsed":9166,"user":{"displayName":"Manos Panagiotou","photoUrl":"","userId":"06697745822174955863"}}},"source":["\"\"\" Module implementing GAN which will be trained using the Progressive growing\n","    technique -> https://arxiv.org/abs/1710.10196\n","\"\"\"\n","import datetime\n","import os\n","import time\n","import timeit\n","import copy\n","import numpy as np\n","import torch as th\n","\n","\n","# ========================================================================================\n","# Generator Module\n","# can be used with ProGAN, ConditionalProGAN or standalone (for inference)\n","# ========================================================================================\n","\n","class Generator(th.nn.Module):\n","    \"\"\" Generator of the GAN network \"\"\"\n","\n","    def __init__(self, depth=7, latent_size=512, use_eql=True):\n","        \"\"\"\n","        constructor for the Generator class\n","        :param depth: required depth of the Network\n","        :param latent_size: size of the latent manifold\n","        :param use_eql: whether to use equalized learning rate\n","        \"\"\"\n","        from torch.nn import ModuleList\n","        from torch.nn.functional import interpolate\n","\n","        super(Generator, self).__init__()\n","\n","        assert latent_size != 0 and ((latent_size & (latent_size - 1)) == 0), \\\n","            \"latent size not a power of 2\"\n","        if depth >= 4:\n","            assert latent_size >= np.power(2, depth - 4), \"latent size will diminish to zero\"\n","\n","        # state of the generator:\n","        self.use_eql = use_eql\n","        self.depth = depth\n","        self.latent_size = latent_size\n","\n","        # register the modules required for the GAN\n","        self.initial_block = GenInitialBlock(self.latent_size, use_eql=self.use_eql)\n","\n","        # create a module list of the other required general convolution blocks\n","        self.layers = ModuleList([])  # initialize to empty list\n","\n","        # create the ToRGB layers for various outputs:\n","        if self.use_eql:\n","\n","            self.toRGB = lambda in_channels: \\\n","                _equalized_conv2d(in_channels, 3, (1, 1), bias=True)\n","        else:\n","            from torch.nn import Conv2d\n","            self.toRGB = lambda in_channels: Conv2d(in_channels, 3, (1, 1), bias=True)\n","\n","        self.rgb_converters = ModuleList([self.toRGB(self.latent_size)])\n","\n","        # create the remaining layers\n","        for i in range(self.depth - 1):\n","            if i <= 2:\n","                layer = GenGeneralConvBlock(self.latent_size,\n","                                            self.latent_size, use_eql=self.use_eql)\n","                rgb = self.toRGB(self.latent_size)\n","            else:\n","                layer = GenGeneralConvBlock(\n","                    int(self.latent_size // np.power(2, i - 3)),\n","                    int(self.latent_size // np.power(2, i - 2)),\n","                    use_eql=self.use_eql\n","                )\n","                rgb = self.toRGB(int(self.latent_size // np.power(2, i - 2)))\n","            self.layers.append(layer)\n","            self.rgb_converters.append(rgb)\n","\n","        # register the temporary upsampler\n","        self.temporaryUpsampler = lambda x: interpolate(x, scale_factor=2)\n","\n","    def forward(self, x, depth, alpha):\n","        \"\"\"\n","        forward pass of the Generator\n","        :param x: input noise\n","        :param depth: current depth from where output is required\n","        :param alpha: value of alpha for fade-in effect\n","        :return: y => output\n","        \"\"\"\n","\n","        assert depth < self.depth, \"Requested output depth cannot be produced\"\n","\n","        y = self.initial_block(x)\n","\n","        if depth > 0:\n","            for block in self.layers[:depth - 1]:\n","                y = block(y)\n","\n","            residual = self.rgb_converters[depth - 1](self.temporaryUpsampler(y))\n","            straight = self.rgb_converters[depth](self.layers[depth - 1](y))\n","\n","            out = (alpha * straight) + ((1 - alpha) * residual)\n","\n","        else:\n","            out = self.rgb_converters[0](y)\n","\n","        return out\n","\n","\n","# ========================================================================================\n","# Discriminator Module\n","# can be used with ProGAN or standalone (for inference).\n","# Note this cannot be used with ConditionalProGAN\n","# ========================================================================================\n","\n","class Discriminator(th.nn.Module):\n","    \"\"\" Discriminator of the GAN \"\"\"\n","\n","    def __init__(self, height=7, feature_size=512, use_eql=True):\n","        \"\"\"\n","        constructor for the class\n","        :param height: total height of the discriminator (Must be equal to the Generator depth)\n","        :param feature_size: size of the deepest features extracted\n","                             (Must be equal to Generator latent_size)\n","        :param use_eql: whether to use equalized learning rate\n","        \"\"\"\n","        from torch.nn import ModuleList, AvgPool2d\n","\n","        super(Discriminator, self).__init__()\n","\n","        assert feature_size != 0 and ((feature_size & (feature_size - 1)) == 0), \\\n","            \"latent size not a power of 2\"\n","        if height >= 4:\n","            assert feature_size >= np.power(2, height - 4), \"feature size cannot be produced\"\n","\n","        # create state of the object\n","        self.use_eql = use_eql\n","        self.height = height\n","        self.feature_size = feature_size\n","\n","        self.final_block = DisFinalBlock(self.feature_size, use_eql=self.use_eql)\n","\n","        # create a module list of the other required general convolution blocks\n","        self.layers = ModuleList([])  # initialize to empty list\n","\n","        # create the fromRGB layers for various inputs:\n","        if self.use_eql:\n","            self.fromRGB = lambda out_channels: \\\n","                _equalized_conv2d(3, out_channels, (1, 1), bias=True)\n","        else:\n","            from torch.nn import Conv2d\n","            self.fromRGB = lambda out_channels: Conv2d(3, out_channels, (1, 1), bias=True)\n","\n","        self.rgb_to_features = ModuleList([self.fromRGB(self.feature_size)])\n","\n","        # create the remaining layers\n","        for i in range(self.height - 1):\n","            if i > 2:\n","                layer = DisGeneralConvBlock(\n","                    int(self.feature_size // np.power(2, i - 2)),\n","                    int(self.feature_size // np.power(2, i - 3)),\n","                    use_eql=self.use_eql\n","                )\n","                rgb = self.fromRGB(int(self.feature_size // np.power(2, i - 2)))\n","            else:\n","                layer = DisGeneralConvBlock(self.feature_size,\n","                                            self.feature_size, use_eql=self.use_eql)\n","                rgb = self.fromRGB(self.feature_size)\n","\n","            self.layers.append(layer)\n","            self.rgb_to_features.append(rgb)\n","\n","        # register the temporary downSampler\n","        self.temporaryDownsampler = AvgPool2d(2)\n","\n","    def forward(self, x, height, alpha):\n","        \"\"\"\n","        forward pass of the discriminator\n","        :param x: input to the network\n","        :param height: current height of operation (Progressive GAN)\n","        :param alpha: current value of alpha for fade-in\n","        :return: out => raw prediction values (WGAN-GP)\n","        \"\"\"\n","\n","        assert height < self.height, \"Requested output depth cannot be produced\"\n","\n","        if height > 0:\n","            residual = self.rgb_to_features[height - 1](self.temporaryDownsampler(x))\n","\n","            straight = self.layers[height - 1](\n","                self.rgb_to_features[height](x)\n","            )\n","\n","            y = (alpha * straight) + ((1 - alpha) * residual)\n","\n","            for block in reversed(self.layers[:height - 1]):\n","                y = block(y)\n","        else:\n","            y = self.rgb_to_features[0](x)\n","\n","        out = self.final_block(y)\n","\n","        return out\n","\n","\n","# ========================================================================================\n","# ConditionalDiscriminator Module\n","# uses the projection discrimination mechanism\n","# can be used with ConditionalProGAN or standalone (for inference)\n","# Note that this is not to be used with ProGAN\n","# ========================================================================================\n","\n","class ConditionalDiscriminator(th.nn.Module):\n","    \"\"\" Discriminator of the GAN \"\"\"\n","\n","    def __init__(self, num_classes, height=7, feature_size=512, use_eql=True):\n","        \"\"\"\n","        constructor for the class\n","        :param num_classes: number of classes for conditional discrimination\n","        :param height: total height of the discriminator (Must be equal to the Generator depth)\n","        :param feature_size: size of the deepest features extracted\n","                             (Must be equal to Generator latent_size)\n","        :param use_eql: whether to use equalized learning rate\n","        \"\"\"\n","        from torch.nn import ModuleList, AvgPool2d\n","\n","        super(ConditionalDiscriminator, self).__init__()\n","\n","        assert feature_size != 0 and ((feature_size & (feature_size - 1)) == 0), \\\n","            \"latent size not a power of 2\"\n","        if height >= 4:\n","            assert feature_size >= np.power(2, height - 4), \"feature size cannot be produced\"\n","\n","        # create state of the object\n","        self.use_eql = use_eql\n","        self.height = height\n","        self.feature_size = feature_size\n","        self.num_classes = num_classes\n","\n","        self.final_block = ConDisFinalBlock(self.feature_size, self.num_classes,\n","                                            use_eql=self.use_eql)\n","\n","        # create a module list of the other required general convolution blocks\n","        self.layers = ModuleList([])  # initialize to empty list\n","\n","        # create the fromRGB layers for various inputs:\n","        if self.use_eql:\n","            self.fromRGB = lambda out_channels: \\\n","                _equalized_conv2d(3, out_channels, (1, 1), bias=True)\n","        else:\n","            from torch.nn import Conv2d\n","            self.fromRGB = lambda out_channels: Conv2d(3, out_channels, (1, 1), bias=True)\n","\n","        self.rgb_to_features = ModuleList([self.fromRGB(self.feature_size)])\n","\n","        # create the remaining layers\n","        for i in range(self.height - 1):\n","            if i > 2:\n","                layer = DisGeneralConvBlock(\n","                    int(self.feature_size // np.power(2, i - 2)),\n","                    int(self.feature_size // np.power(2, i - 3)),\n","                    use_eql=self.use_eql\n","                )\n","                rgb = self.fromRGB(int(self.feature_size // np.power(2, i - 2)))\n","            else:\n","                layer = DisGeneralConvBlock(self.feature_size,\n","                                            self.feature_size, use_eql=self.use_eql)\n","                rgb = self.fromRGB(self.feature_size)\n","\n","            self.layers.append(layer)\n","            self.rgb_to_features.append(rgb)\n","\n","        # register the temporary downSampler\n","        self.temporaryDownsampler = AvgPool2d(2)\n","\n","    def forward(self, x, labels, height, alpha):\n","        \"\"\"\n","        forward pass of the discriminator\n","        :param x: input to the network\n","        :param labels: labels required for conditional discrimination\n","                       note that these are pure integer labels of shape [B x 1]\n","        :param height: current height of operation (Progressive GAN)\n","        :param alpha: current value of alpha for fade-in\n","        :return: out => raw prediction values\n","        \"\"\"\n","\n","        assert height < self.height, \"Requested output depth cannot be produced\"\n","\n","        if height > 0:\n","            residual = self.rgb_to_features[height - 1](self.temporaryDownsampler(x))\n","\n","            straight = self.layers[height - 1](\n","                self.rgb_to_features[height](x)\n","            )\n","\n","            y = (alpha * straight) + ((1 - alpha) * residual)\n","\n","            for block in reversed(self.layers[:height - 1]):\n","                y = block(y)\n","        else:\n","            y = self.rgb_to_features[0](x)\n","\n","        out = self.final_block(y, labels)\n","\n","        return out\n","\n","\n","# ========================================================================================\n","# ProGAN Module (Unconditional)\n","# ========================================================================================\n","\n","class ProGAN:\n","    \"\"\" Wrapper around the Generator and the Discriminator \"\"\"\n","\n","    def __init__(self, depth=7, latent_size=512, learning_rate=0.001, beta_1=0,\n","                 beta_2=0.99, eps=1e-8, drift=0.001, n_critic=1, use_eql=True,\n","                 loss=\"wgan-gp\", use_ema=True, ema_decay=0.999,\n","                 device=th.device(\"cpu\")):\n","        \"\"\"\n","        constructor for the class\n","        :param depth: depth of the GAN (will be used for each generator and discriminator)\n","        :param latent_size: latent size of the manifold used by the GAN\n","        :param learning_rate: learning rate for Adam\n","        :param beta_1: beta_1 for Adam\n","        :param beta_2: beta_2 for Adam\n","        :param eps: epsilon for Adam\n","        :param n_critic: number of times to update discriminator per generator update\n","        :param drift: drift penalty for the\n","                      (Used only if loss is wgan or wgan-gp)\n","        :param use_eql: whether to use equalized learning rate\n","        :param loss: the loss function to be used\n","                     Can either be a string =>\n","                          [\"wgan-gp\", \"wgan\", \"lsgan\", \"lsgan-with-sigmoid\",\n","                          \"hinge\", \"standard-gan\" or \"relativistic-hinge\"]\n","                     Or an instance of GANLoss\n","        :param use_ema: boolean for whether to use exponential moving averages\n","        :param ema_decay: value of mu for ema\n","        :param device: device to run the GAN on (GPU / CPU)\n","        \"\"\"\n","\n","        from torch.optim import Adam\n","        from torch.nn import DataParallel\n","\n","        # Create the Generator and the Discriminator\n","        self.gen = Generator(depth, latent_size, use_eql=use_eql).to(device)\n","        self.dis = Discriminator(depth, latent_size, use_eql=use_eql).to(device)\n","\n","        # if code is to be run on GPU, we can use DataParallel:\n","        if device == th.device(\"cuda\"):\n","            self.gen = DataParallel(self.gen)\n","            self.dis = DataParallel(self.dis)\n","\n","        # state of the object\n","        self.latent_size = latent_size\n","        self.depth = depth\n","        self.use_ema = use_ema\n","        self.ema_decay = ema_decay\n","        self.n_critic = n_critic\n","        self.use_eql = use_eql\n","        self.device = device\n","        self.drift = drift\n","\n","        # define the optimizers for the discriminator and generator\n","        self.gen_optim = Adam(self.gen.parameters(), lr=learning_rate,\n","                              betas=(beta_1, beta_2), eps=eps)\n","\n","        self.dis_optim = Adam(self.dis.parameters(), lr=learning_rate,\n","                              betas=(beta_1, beta_2), eps=eps)\n","\n","        # define the loss function used for training the GAN\n","        self.loss = self.__setup_loss(loss)\n","\n","        if self.use_ema:\n","\n","            # create a shadow copy of the generator\n","            self.gen_shadow = copy.deepcopy(self.gen)\n","\n","            # updater function:\n","            self.ema_updater = update_average\n","\n","            # initialize the gen_shadow weights equal to the\n","            # weights of gen\n","            self.ema_updater(self.gen_shadow, self.gen, beta=0)\n","\n","    def __setup_loss(self, loss):\n","\n","        if isinstance(loss, str):\n","            loss = loss.lower()  # lowercase the string\n","            if loss == \"wgan\":\n","                loss = WGAN_GP(self.dis, self.drift, use_gp=False)\n","                # note if you use just wgan, you will have to use weight clipping\n","                # in order to prevent gradient exploding\n","                # check the optimize_discriminator method where this has been\n","                # taken care of.\n","\n","            elif loss == \"wgan-gp\":\n","                loss = WGAN_GP(self.dis, self.drift, use_gp=True)\n","\n","            elif loss == \"standard-gan\":\n","                loss = StandardGAN(self.dis)\n","\n","            elif loss == \"lsgan\":\n","                loss = LSGAN(self.dis)\n","\n","            elif loss == \"lsgan-with-sigmoid\":\n","                loss = LSGAN_SIGMOID(self.dis)\n","\n","            elif loss == \"hinge\":\n","                loss = HingeGAN(self.dis)\n","\n","            elif loss == \"relativistic-hinge\":\n","                loss = RelativisticAverageHingeGAN(self.dis)\n","\n","            else:\n","                raise ValueError(\"Unknown loss function requested\")\n","\n","        elif not isinstance(loss, GANLoss):\n","            raise ValueError(\"loss is neither an instance of GANLoss nor a string\")\n","\n","        return loss\n","\n","    def __progressive_downsampling(self, real_batch, depth, alpha):\n","        \"\"\"\n","        private helper for downsampling the original images in order to facilitate the\n","        progressive growing of the layers.\n","        :param real_batch: batch of real samples\n","        :param depth: depth at which training is going on\n","        :param alpha: current value of the fader alpha\n","        :return: real_samples => modified real batch of samples\n","        \"\"\"\n","\n","        from torch.nn import AvgPool2d\n","        from torch.nn.functional import interpolate\n","\n","        # downsample the real_batch for the given depth\n","        down_sample_factor = int(np.power(2, self.depth - depth - 1))\n","        prior_downsample_factor = max(int(np.power(2, self.depth - depth)), 0)\n","\n","        ds_real_samples = AvgPool2d(down_sample_factor)(real_batch)\n","\n","        if depth > 0:\n","            prior_ds_real_samples = interpolate(AvgPool2d(prior_downsample_factor)(real_batch),\n","                                                scale_factor=2)\n","        else:\n","            prior_ds_real_samples = ds_real_samples\n","\n","        # real samples are a combination of ds_real_samples and prior_ds_real_samples\n","        real_samples = (alpha * ds_real_samples) + ((1 - alpha) * prior_ds_real_samples)\n","\n","        # return the so computed real_samples\n","        return real_samples\n","\n","    def optimize_discriminator(self, noise, real_batch, depth, alpha):\n","        \"\"\"\n","        performs one step of weight update on discriminator using the batch of data\n","        :param noise: input noise of sample generation\n","        :param real_batch: real samples batch\n","        :param depth: current depth of optimization\n","        :param alpha: current alpha for fade-in\n","        :return: current loss (Wasserstein loss)\n","        \"\"\"\n","\n","        real_samples = self.__progressive_downsampling(real_batch, depth, alpha)\n","\n","        loss_val = 0\n","        for _ in range(self.n_critic):\n","            # generate a batch of samples\n","            fake_samples = self.gen(noise, depth, alpha).detach()\n","\n","            loss = self.loss.dis_loss(real_samples, fake_samples, depth, alpha)\n","\n","            # optimize discriminator\n","            self.dis_optim.zero_grad()\n","            loss.backward()\n","            self.dis_optim.step()\n","\n","            loss_val += loss.item()\n","\n","        return loss_val / self.n_critic\n","\n","    def optimize_generator(self, noise, real_batch, depth, alpha):\n","        \"\"\"\n","        performs one step of weight update on generator for the given batch_size\n","        :param noise: input random noise required for generating samples\n","        :param real_batch: batch of real samples\n","        :param depth: depth of the network at which optimization is done\n","        :param alpha: value of alpha for fade-in effect\n","        :return: current loss (Wasserstein estimate)\n","        \"\"\"\n","\n","        real_samples = self.__progressive_downsampling(real_batch, depth, alpha)\n","\n","        # generate fake samples:\n","        fake_samples = self.gen(noise, depth, alpha)\n","\n","        # TODO_complete:\n","        # Change this implementation for making it compatible for relativisticGAN\n","        loss = self.loss.gen_loss(real_samples, fake_samples, depth, alpha)\n","\n","        # optimize the generator\n","        self.gen_optim.zero_grad()\n","        loss.backward()\n","        self.gen_optim.step()\n","\n","        # if use_ema is true, apply ema to the generator parameters\n","        if self.use_ema:\n","            self.ema_updater(self.gen_shadow, self.gen, self.ema_decay)\n","\n","        # return the loss value\n","        return loss.item()\n","\n","    @staticmethod\n","    def create_grid(samples, scale_factor, img_file):\n","        \"\"\"\n","        utility function to create a grid of GAN samples\n","        :param samples: generated samples for storing\n","        :param scale_factor: factor for upscaling the image\n","        :param img_file: name of file to write\n","        :return: None (saves a file)\n","        \"\"\"\n","        from torchvision.utils import save_image\n","        from torch.nn.functional import interpolate\n","\n","        # upsample the image\n","        if scale_factor > 1:\n","            samples = interpolate(samples, scale_factor=scale_factor)\n","\n","        # save the images:\n","        save_image(samples, img_file, nrow=int(np.sqrt(len(samples))),\n","                   normalize=True, scale_each=True)\n","\n","    def train(self, dataset, epochs, batch_sizes,\n","              fade_in_percentage, num_samples=16,\n","              start_depth=0, start_epoch=1, num_workers=3, feedback_factor=100,\n","              log_dir=\"./models/\", sample_dir=\"./samples/\", save_dir=\"./models/\",\n","              checkpoint_factor=1):\n","        \"\"\"\n","        Utility method for training the ProGAN. Note that you don't have to necessarily use this\n","        you can use the optimize_generator and optimize_discriminator for your own training routine.\n","        :param dataset: object of the dataset used for training.\n","                        Note that this is not the dataloader (we create dataloader in this method\n","                        since the batch_sizes for resolutions can be different)\n","        :param epochs: list of number of epochs to train the network for every resolution\n","        :param batch_sizes: list of batch_sizes for every resolution\n","        :param fade_in_percentage: list of percentages of epochs per resolution\n","                                   used for fading in the new layer\n","                                   not used for first resolution, but dummy value still needed.\n","        :param num_samples: number of samples generated in sample_sheet. def=36\n","        :param start_depth: start training from this depth. def=0\n","        :param num_workers: number of workers for reading the data. def=3\n","        :param feedback_factor: number of logs per epoch. def=100\n","        :param log_dir: directory for saving the loss logs. def=\"./models/\"\n","        :param sample_dir: directory for saving the generated samples. def=\"./samples/\"\n","        :param checkpoint_factor: save model after these many epochs.\n","                                  Note that only one model is stored per resolution.\n","                                  during one resolution, the checkpoint will be updated (Rewritten)\n","                                  according to this factor.\n","        :param save_dir: directory for saving the models (.pth files)\n","        :return: None (Writes multiple files to disk)\n","        \"\"\"\n","\n","        assert self.depth == len(batch_sizes), \"batch_sizes not compatible with depth\"\n","\n","        #RELOAD\n","        gen_save_file = os.path.join(save_dir, \"GAN_GEN_\" + str(start_depth) + \".pth\")\n","        dis_save_file = os.path.join(save_dir, \"GAN_DIS_\" + str(start_depth) + \".pth\")\n","        gen_optim_save_file = os.path.join(save_dir,\n","                                            \"GAN_GEN_OPTIM_\" + str(start_depth)\n","                                            + \".pth\")\n","        dis_optim_save_file = os.path.join(save_dir,\n","                                            \"GAN_DIS_OPTIM_\" + str(start_depth)\n","                                            + \".pth\")\n","        self.gen.load_state_dict(th.load(gen_save_file))\n","        self.dis.load_state_dict(th.load(dis_save_file))\n","        self.gen_optim.load_state_dict(th.load(gen_optim_save_file))\n","        self.dis_optim.load_state_dict(th.load(dis_optim_save_file))\n","\n","        # also save the shadow generator if use_ema is True\n","        if self.use_ema:\n","            gen_shadow_save_file = os.path.join(save_dir, \"GAN_GEN_SHADOW_\" +\n","                                                str(start_depth) + \".pth\")\n","            self.gen_shadow.load_state_dict(th.load(gen_shadow_save_file))\n","        #RELOAD\n","\n","        # turn the generator and discriminator into train mode\n","        self.gen.train()\n","        self.dis.train()\n","        if self.use_ema:\n","            self.gen_shadow.train()\n","\n","        # create a global time counter\n","        global_time = time.time()\n","\n","        # create fixed_input for debugging\n","        fixed_input = th.randn(num_samples, self.latent_size).to(self.device)\n","\n","        print(\"Starting the training process ... \")\n","        for current_depth in range(start_depth, self.depth):\n","\n","\n","            print(\"\\n\\nCurrently working on Depth: \", current_depth)\n","            current_res = np.power(2, current_depth + 2)\n","            print(\"Current resolution: %d x %d\" % (current_res, current_res))\n","\n","            data = get_data_loader(dataset, batch_sizes[current_depth], num_workers)\n","            if (start_epoch > 1): ticker = len(range(1,start_epoch))* len(iter(data))\n","            else: ticker = 1\n","            print(\"Ticker\", ticker)\n","\n","            for epoch in range(start_epoch, epochs[current_depth] + 1):\n","                start = timeit.default_timer()  # record time at the start of epoch\n","\n","                print(\"\\nEpoch: %d\" % epoch)\n","                total_batches = len(iter(data))\n","\n","                fader_point = int((fade_in_percentage[current_depth] / 100)\n","                                  * epochs[current_depth] * total_batches)\n","\n","                step = 0  # counter for number of iterations\n","\n","                for (i, batch) in enumerate(data, 1):\n","                    # calculate the alpha for fading in the layers\n","                    alpha = ticker / fader_point if ticker <= fader_point else 1\n","\n","                    # extract current batch of data for training\n","                    images = batch[0].to(self.device)\n","\n","                    gan_input = th.randn(images.shape[0], self.latent_size).to(self.device)\n","\n","                    # optimize the discriminator:\n","                    dis_loss = self.optimize_discriminator(gan_input, images,\n","                                                           current_depth, alpha)\n","\n","                    # optimize the generator:\n","                    gen_loss = self.optimize_generator(gan_input, images, current_depth, alpha)\n","\n","                    # provide a loss feedback\n","                    if i % int(total_batches / feedback_factor) == 0 or i == 1:\n","                        elapsed = time.time() - global_time\n","                        elapsed = str(datetime.timedelta(seconds=elapsed))\n","                        print(\"Elapsed: [%s]  batch: %d  d_loss: %f  g_loss: %f\"\n","                              % (elapsed, i, dis_loss, gen_loss))\n","\n","                        # also write the losses to the log file:\n","                        os.makedirs(log_dir, exist_ok=True)\n","                        log_file = os.path.join(log_dir, \"loss_\" + str(current_depth) + \".log\")\n","                        with open(log_file, \"a\") as log:\n","                            log.write(str(step) + \"\\t\" + str(dis_loss) +\n","                                      \"\\t\" + str(gen_loss) + \"\\n\")\n","                    # increment the alpha ticker and the step\n","                    ticker += 1\n","                    step += 1\n","                # create a grid of samples and save it\n","                os.makedirs(sample_dir, exist_ok=True)\n","                gen_img_file = os.path.join(sample_dir, \"gen_\" + str(current_depth) +\n","                                            \"_\" + str(epoch) + \"_\" +\n","                                            str(i) + \".png\")\n","\n","                # this is done to allow for more GPU space\n","                with th.no_grad():\n","                    self.create_grid(\n","                        samples=self.gen(\n","                            fixed_input,\n","                            current_depth,\n","                            alpha\n","                        ).detach() if not self.use_ema\n","                        else self.gen_shadow(\n","                            fixed_input,\n","                            current_depth,\n","                            alpha\n","                        ).detach(),\n","                        scale_factor=int(np.power(2, self.depth - current_depth - 1)),\n","                        img_file=gen_img_file,\n","                    )\n","\n","                stop = timeit.default_timer()\n","                print(\"Time taken for epoch: %.3f secs\" % (stop - start))\n","                print(\"ticker = \", ticker)\n","                if epoch % checkpoint_factor == 0 or epoch == 1 or epoch == epochs[current_depth]:\n","                    os.makedirs(save_dir, exist_ok=True)\n","                    gen_save_file = os.path.join(save_dir, \"GAN_GEN_\" + str(current_depth) + \".pth\")\n","                    dis_save_file = os.path.join(save_dir, \"GAN_DIS_\" + str(current_depth) + \".pth\")\n","                    gen_optim_save_file = os.path.join(save_dir,\n","                                                       \"GAN_GEN_OPTIM_\" + str(current_depth)\n","                                                       + \".pth\")\n","                    dis_optim_save_file = os.path.join(save_dir,\n","                                                       \"GAN_DIS_OPTIM_\" + str(current_depth)\n","                                                       + \".pth\")\n","\n","                    th.save(self.gen.state_dict(), gen_save_file)\n","                    th.save(self.dis.state_dict(), dis_save_file)\n","                    th.save(self.gen_optim.state_dict(), gen_optim_save_file)\n","                    th.save(self.dis_optim.state_dict(), dis_optim_save_file)\n","\n","                    # also save the shadow generator if use_ema is True\n","                    if self.use_ema:\n","                        gen_shadow_save_file = os.path.join(save_dir, \"GAN_GEN_SHADOW_\" +\n","                                                            str(current_depth) + \".pth\")\n","                        th.save(self.gen_shadow.state_dict(), gen_shadow_save_file)\n","\n","        # put the gen, shadow_gen and dis in eval mode\n","        self.gen.eval()\n","        self.dis.eval()\n","        if self.use_ema:\n","            self.gen_shadow.eval()\n","\n","        print(\"Training completed ...\")"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"z3L3ciAgDP1a","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1593966903198,"user_tz":-180,"elapsed":8760582,"user":{"displayName":"Manos Panagiotou","photoUrl":"","userId":"06697745822174955863"}},"outputId":"0cc0cdc2-0fbe-472e-85c5-86968ae7a4e4"},"source":["import torch as th\n","import torchvision as tv\n","from google.colab import drive\n","from google.colab import files\n","from torch.utils.data import TensorDataset\n","# select the device to be used for training\n","device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n","\n","def npy_loader(path):\n","    arr = np.load(path)['arr_0']\n","    listt = np.array([np.reshape(x/127.5 - 1, (3, 256, 256))  for x in arr[:1000]])\n","    sample = th.from_numpy(listt).type(th.float32) \n","    del listt\n","    return sample\n","    \n","\n","\n","\n","def setup_data(download=False):\n","    \"\"\"\n","    setup the CIFAR-10 dataset for training the CNN\n","    :param batch_size: batch_size for sgd\n","    :param num_workers: num_readers for data reading\n","    :param download: Boolean for whether to download the data\n","    :return: classes, trainloader, testloader => training and testing data loaders\n","    \"\"\"\n","    # data setup:\n","    classes = ('plane', 'car', 'bird', 'cat', 'deer',\n","               'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","    transforms = tv.transforms.ToTensor()\n","\n","    trainset = tv.datasets.ImageFolder(root=data_path,\n","                                   transform=transforms)\n","\n","    testset = tv.datasets.ImageFolder(root=data_path,\n","                                  transform=transforms)\n","    # trainset = tv.datasets.CIFAR10(root=data_path,\n","    #                                transform=transforms,\n","    #                                download=download)\n","\n","    # testset = tv.datasets.CIFAR10(root=data_path,\n","    #                               transform=transforms, train=False,\n","    #                               download=False)\n","    # trainset = tv.datasets.DatasetFolder(\n","    #     root=data_path,\n","    #     loader=npy_loader,\n","    #     extensions='.npz'\n","    # )\n","    # trainset =  TensorDataset(npy_loader(data_path +\"/sub/Train.npz\"))\n","    # testset = None\n","    return classes, trainset, testset\n","\n","\n","if __name__ == '__main__':\n","    drive.mount('/content/drive')\n","    path = \"/content/drive/My Drive/ImageToDEM/\"\n","    data_path = path + \"DATA5\"\n","\n","    # some parameters:\n","    depth = 7\n","    START_DEPTH = 6\n","    START_EPOCH = 53\n","    # hyper-parameters per depth (resolution)\n","    num_epochs = [50, 60, 70, 100, 150, 200, 300]\n","    fade_ins = [50, 50, 50, 50, 50, 50, 50]\n","    batch_sizes = depth*[8]\n","    latent_size = 256\n","    # get the data. Ignore the test data and their classes\n","    _, dataset, _ = setup_data(download=True)\n","    print(dataset)\n","    print([dataset[i][0].shape for i in range(1)])\n","    # dataset1 = np.load(path + '/Data/Train_5cm_norm.npz')\n","    # dataset2 = dataset1['arr_0']\n","    # print(dataset2.shape)\n","    # dataset = TensorDataset(*dataset2)\n","    # ======================================================================\n","    # This line creates the PRO-GAN\n","    # ======================================================================\n","    # pro_gan = ConditionalProGAN(num_classes=10, depth=depth, \n","    #                                latent_size=latent_size, device=device)\n","    pro_gan = ProGAN(depth=depth, \n","                                   latent_size=latent_size, device=device)\n","    # ======================================================================\n","\n","    # ======================================================================\n","    # This line trains the PRO-GAN\n","    # ======================================================================\n","    pro_gan.train(\n","        dataset=dataset,\n","        epochs=num_epochs,\n","        fade_in_percentage=fade_ins,\n","        batch_sizes=batch_sizes,\n","        log_dir=path + \"/models/\", sample_dir=path + \"/samples/\", save_dir= path + \"/models/\",\n","        start_depth=START_DEPTH,\n","        start_epoch=START_EPOCH\n","    )\n","    # ======================================================================"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","\n","Mounted at /content/drive\n","Dataset ImageFolder\n","    Number of datapoints: 10444\n","    Root location: /content/drive/My Drive/ImageToDEM/DATA5\n","    StandardTransform\n","Transform: ToTensor()\n","[torch.Size([3, 256, 256])]\n","Starting the training process ... \n","\n","\n","Currently working on Depth:  6\n","Current resolution: 256 x 256\n","Ticker 67912\n","\n","Epoch: 53\n","Elapsed: [0:00:11.292424]  batch: 1  d_loss: -3.442550  g_loss: 16.112728\n","Elapsed: [0:00:47.960469]  batch: 13  d_loss: -3.803960  g_loss: 16.758366\n","Elapsed: [0:01:27.765289]  batch: 26  d_loss: -19.924728  g_loss: 24.416677\n","Elapsed: [0:02:07.922675]  batch: 39  d_loss: -11.233962  g_loss: 22.271511\n","Elapsed: [0:02:48.189894]  batch: 52  d_loss: -1.946685  g_loss: 18.670870\n","Elapsed: [0:03:28.401743]  batch: 65  d_loss: -11.583139  g_loss: 18.822720\n","Elapsed: [0:04:08.598965]  batch: 78  d_loss: -16.167772  g_loss: 25.470602\n","Elapsed: [0:04:48.878305]  batch: 91  d_loss: -9.649570  g_loss: 18.824009\n","Elapsed: [0:05:29.121367]  batch: 104  d_loss: -2.362917  g_loss: 14.271999\n","Elapsed: [0:06:09.330566]  batch: 117  d_loss: -10.828426  g_loss: 14.377446\n","Elapsed: [0:06:49.520506]  batch: 130  d_loss: 2.600054  g_loss: 16.230436\n","Elapsed: [0:07:29.824177]  batch: 143  d_loss: -2.530052  g_loss: 14.889091\n","Elapsed: [0:08:10.153343]  batch: 156  d_loss: -0.787879  g_loss: 4.851568\n","Elapsed: [0:08:50.373356]  batch: 169  d_loss: -16.937235  g_loss: 23.502611\n","Elapsed: [0:09:30.590298]  batch: 182  d_loss: -0.105903  g_loss: 20.337215\n","Elapsed: [0:10:10.939181]  batch: 195  d_loss: -5.848864  g_loss: 26.267395\n","Elapsed: [0:10:51.284194]  batch: 208  d_loss: 6.255014  g_loss: 17.417505\n","Elapsed: [0:11:31.564438]  batch: 221  d_loss: -2.186870  g_loss: 17.456553\n","Elapsed: [0:12:11.785729]  batch: 234  d_loss: -13.553864  g_loss: 14.306089\n","Elapsed: [0:12:52.128401]  batch: 247  d_loss: -17.402496  g_loss: 13.185125\n","Elapsed: [0:13:32.411674]  batch: 260  d_loss: -16.613966  g_loss: 21.397995\n","Elapsed: [0:14:12.655679]  batch: 273  d_loss: -9.074003  g_loss: 12.194304\n","Elapsed: [0:14:52.933381]  batch: 286  d_loss: -7.062506  g_loss: 15.106105\n","Elapsed: [0:15:33.348732]  batch: 299  d_loss: -3.087456  g_loss: 19.621559\n","Elapsed: [0:16:25.085025]  batch: 312  d_loss: -4.568531  g_loss: 14.078648\n","Elapsed: [0:17:05.234344]  batch: 325  d_loss: -1.659753  g_loss: 12.125895\n","Elapsed: [0:17:45.488461]  batch: 338  d_loss: -7.309583  g_loss: 21.325478\n","Elapsed: [0:18:25.810699]  batch: 351  d_loss: 5.128967  g_loss: 13.499455\n","Elapsed: [0:19:06.420580]  batch: 364  d_loss: -9.542017  g_loss: 22.812082\n","Elapsed: [0:19:46.682574]  batch: 377  d_loss: -5.384377  g_loss: 16.341667\n","Elapsed: [0:20:26.917640]  batch: 390  d_loss: -14.310628  g_loss: 5.968926\n","Elapsed: [0:21:07.275810]  batch: 403  d_loss: 5.033168  g_loss: 17.502497\n","Elapsed: [0:21:47.669170]  batch: 416  d_loss: -1.806209  g_loss: 13.524086\n","Elapsed: [0:22:27.946390]  batch: 429  d_loss: -11.384396  g_loss: 16.262814\n","Elapsed: [0:23:08.249374]  batch: 442  d_loss: -11.247590  g_loss: 17.089315\n","Elapsed: [0:23:48.586231]  batch: 455  d_loss: -5.534478  g_loss: 17.344109\n","Elapsed: [0:24:28.874803]  batch: 468  d_loss: -9.422750  g_loss: 14.216994\n","Elapsed: [0:25:09.114548]  batch: 481  d_loss: -2.292040  g_loss: 9.002199\n","Elapsed: [0:25:49.359603]  batch: 494  d_loss: 0.282423  g_loss: 21.464825\n","Elapsed: [0:26:29.678525]  batch: 507  d_loss: -17.849854  g_loss: 17.155846\n","Elapsed: [0:27:10.056369]  batch: 520  d_loss: -8.719567  g_loss: 23.401249\n","Elapsed: [0:27:50.337986]  batch: 533  d_loss: -24.952087  g_loss: 21.625465\n","Elapsed: [0:28:30.635591]  batch: 546  d_loss: -7.572228  g_loss: 12.379818\n","Elapsed: [0:29:10.955841]  batch: 559  d_loss: -3.917746  g_loss: 8.663564\n","Elapsed: [0:29:51.298908]  batch: 572  d_loss: -9.688140  g_loss: 14.524172\n","Elapsed: [0:30:31.548189]  batch: 585  d_loss: -0.845306  g_loss: 19.253466\n","Elapsed: [0:31:11.803745]  batch: 598  d_loss: -9.077056  g_loss: 12.753950\n","Elapsed: [0:31:52.197499]  batch: 611  d_loss: -0.943414  g_loss: 21.889744\n","Elapsed: [0:32:32.551400]  batch: 624  d_loss: -1.590708  g_loss: 7.060382\n","Elapsed: [0:33:12.850796]  batch: 637  d_loss: -2.297811  g_loss: 13.460253\n","Elapsed: [0:33:53.129796]  batch: 650  d_loss: -2.024241  g_loss: 6.561522\n","Elapsed: [0:34:33.466776]  batch: 663  d_loss: -13.970186  g_loss: 17.627926\n","Elapsed: [0:35:13.754652]  batch: 676  d_loss: -16.760742  g_loss: 25.261436\n","Elapsed: [0:35:53.993187]  batch: 689  d_loss: -0.636056  g_loss: 26.460930\n","Elapsed: [0:36:34.265631]  batch: 702  d_loss: -9.480488  g_loss: 25.795364\n","Elapsed: [0:37:14.581739]  batch: 715  d_loss: -5.038013  g_loss: 19.098820\n","Elapsed: [0:37:54.915004]  batch: 728  d_loss: 0.991873  g_loss: 9.138108\n","Elapsed: [0:38:35.201036]  batch: 741  d_loss: -25.455032  g_loss: 21.239166\n","Elapsed: [0:39:15.473992]  batch: 754  d_loss: -6.638616  g_loss: 9.049859\n","Elapsed: [0:39:55.844399]  batch: 767  d_loss: 2.514306  g_loss: 14.971346\n","Elapsed: [0:40:36.162050]  batch: 780  d_loss: -9.349457  g_loss: 8.483315\n","Elapsed: [0:41:16.429835]  batch: 793  d_loss: -6.067466  g_loss: 16.462399\n","Elapsed: [0:41:56.691015]  batch: 806  d_loss: -7.478683  g_loss: 19.701794\n","Elapsed: [0:42:37.058704]  batch: 819  d_loss: -2.960914  g_loss: 10.774742\n","Elapsed: [0:43:17.412030]  batch: 832  d_loss: -3.577279  g_loss: 14.638205\n","Elapsed: [0:43:57.633719]  batch: 845  d_loss: 7.960062  g_loss: 18.052933\n","Elapsed: [0:44:37.885587]  batch: 858  d_loss: -10.185177  g_loss: 17.744770\n","Elapsed: [0:45:18.566032]  batch: 871  d_loss: -4.343214  g_loss: 14.176281\n","Elapsed: [0:45:59.372997]  batch: 884  d_loss: -3.978713  g_loss: 17.448549\n","Elapsed: [0:46:40.004992]  batch: 897  d_loss: -6.676601  g_loss: 17.802555\n","Elapsed: [0:47:20.883884]  batch: 910  d_loss: -4.274148  g_loss: 20.895208\n","Elapsed: [0:48:01.836370]  batch: 923  d_loss: -3.305068  g_loss: 23.764359\n","Elapsed: [0:48:42.814825]  batch: 936  d_loss: -5.788965  g_loss: 13.849470\n","Elapsed: [0:49:23.718580]  batch: 949  d_loss: -22.540125  g_loss: 18.776167\n","Elapsed: [0:50:04.618301]  batch: 962  d_loss: -8.457524  g_loss: 23.923561\n","Elapsed: [0:50:45.607335]  batch: 975  d_loss: -0.201983  g_loss: 20.790752\n","Elapsed: [0:51:26.572261]  batch: 988  d_loss: -5.376966  g_loss: 13.735340\n","Elapsed: [0:52:07.444871]  batch: 1001  d_loss: -3.740999  g_loss: 12.036692\n","Elapsed: [0:52:48.392618]  batch: 1014  d_loss: -6.573587  g_loss: 14.379885\n","Elapsed: [0:53:29.372786]  batch: 1027  d_loss: -13.320767  g_loss: 23.084368\n","Elapsed: [0:54:10.327052]  batch: 1040  d_loss: -11.857061  g_loss: 12.675003\n","Elapsed: [0:54:51.258261]  batch: 1053  d_loss: 0.894690  g_loss: 20.694925\n","Elapsed: [0:55:32.189693]  batch: 1066  d_loss: -5.222134  g_loss: 5.639386\n","Elapsed: [0:56:13.197471]  batch: 1079  d_loss: -15.408467  g_loss: 13.670717\n","Elapsed: [0:56:54.139120]  batch: 1092  d_loss: -20.383099  g_loss: 19.233624\n","Elapsed: [0:57:35.077699]  batch: 1105  d_loss: -16.229080  g_loss: 14.617981\n","Elapsed: [0:58:15.980883]  batch: 1118  d_loss: -14.193462  g_loss: 15.557947\n","Elapsed: [0:58:56.968519]  batch: 1131  d_loss: 0.917932  g_loss: 19.520161\n","Elapsed: [0:59:37.966678]  batch: 1144  d_loss: -8.018602  g_loss: 12.668364\n","Elapsed: [1:00:18.860298]  batch: 1157  d_loss: 1.328747  g_loss: 13.923466\n","Elapsed: [1:00:59.701738]  batch: 1170  d_loss: 4.475931  g_loss: 15.649694\n","Elapsed: [1:01:40.678087]  batch: 1183  d_loss: -7.996247  g_loss: 24.284121\n","Elapsed: [1:02:21.627630]  batch: 1196  d_loss: -14.213384  g_loss: 13.840881\n","Elapsed: [1:03:02.528608]  batch: 1209  d_loss: -2.815125  g_loss: 12.620296\n","Elapsed: [1:03:43.421457]  batch: 1222  d_loss: -13.929962  g_loss: 12.640506\n","Elapsed: [1:04:24.375512]  batch: 1235  d_loss: -5.745703  g_loss: 15.333443\n","Elapsed: [1:05:05.336016]  batch: 1248  d_loss: -3.014275  g_loss: 16.809425\n","Elapsed: [1:05:46.244286]  batch: 1261  d_loss: -7.837238  g_loss: 18.565922\n","Elapsed: [1:06:27.120313]  batch: 1274  d_loss: -16.848970  g_loss: 20.815262\n","Elapsed: [1:07:08.036829]  batch: 1287  d_loss: 0.982138  g_loss: 10.376501\n","Elapsed: [1:07:48.985797]  batch: 1300  d_loss: -3.809078  g_loss: 3.967273\n","Time taken for epoch: 4089.753 secs\n","ticker =  69218\n","\n","Epoch: 54\n","Elapsed: [1:08:17.002802]  batch: 1  d_loss: -12.570539  g_loss: 8.959665\n","Elapsed: [1:08:54.827941]  batch: 13  d_loss: -6.587553  g_loss: 20.245125\n","Elapsed: [1:09:35.736768]  batch: 26  d_loss: -10.417629  g_loss: 11.099583\n","Elapsed: [1:10:16.719598]  batch: 39  d_loss: -8.814303  g_loss: 23.601145\n","Elapsed: [1:10:57.688915]  batch: 52  d_loss: -1.249872  g_loss: 12.789100\n","Elapsed: [1:11:38.577428]  batch: 65  d_loss: -11.923328  g_loss: 13.198997\n","Elapsed: [1:12:19.494791]  batch: 78  d_loss: -10.880640  g_loss: 26.159264\n","Elapsed: [1:13:00.494645]  batch: 91  d_loss: -10.040915  g_loss: 18.364010\n","Elapsed: [1:13:41.510784]  batch: 104  d_loss: -8.738459  g_loss: 23.191896\n","Elapsed: [1:14:22.419714]  batch: 117  d_loss: -11.650549  g_loss: 23.707079\n","Elapsed: [1:15:03.321238]  batch: 130  d_loss: -15.334888  g_loss: 33.080990\n","Elapsed: [1:15:44.318353]  batch: 143  d_loss: -6.180234  g_loss: 18.331602\n","Elapsed: [1:16:25.235493]  batch: 156  d_loss: 0.723880  g_loss: 11.150654\n","Elapsed: [1:17:06.083746]  batch: 169  d_loss: -17.807753  g_loss: 8.638344\n","Elapsed: [1:17:46.930118]  batch: 182  d_loss: 2.060474  g_loss: 12.072070\n","Elapsed: [1:18:27.844964]  batch: 195  d_loss: -9.223841  g_loss: 12.238428\n","Elapsed: [1:19:08.773151]  batch: 208  d_loss: -4.610612  g_loss: 10.552565\n","Elapsed: [1:19:49.626446]  batch: 221  d_loss: -3.641854  g_loss: 12.274242\n","Elapsed: [1:20:30.438258]  batch: 234  d_loss: -7.817658  g_loss: 3.506664\n","Elapsed: [1:21:11.383286]  batch: 247  d_loss: -4.633734  g_loss: 12.510427\n","Elapsed: [1:21:52.358581]  batch: 260  d_loss: -19.253874  g_loss: 9.810185\n","Elapsed: [1:22:33.243444]  batch: 273  d_loss: -2.426265  g_loss: 19.432358\n","Elapsed: [1:23:14.130805]  batch: 286  d_loss: -1.958288  g_loss: 15.483509\n","Elapsed: [1:23:55.053811]  batch: 299  d_loss: 1.563685  g_loss: 12.512808\n","Elapsed: [1:24:35.951859]  batch: 312  d_loss: -9.749854  g_loss: 21.822098\n","Elapsed: [1:25:16.792644]  batch: 325  d_loss: -4.372900  g_loss: 13.307557\n","Elapsed: [1:25:57.641814]  batch: 338  d_loss: -7.366093  g_loss: 16.774048\n","Elapsed: [1:26:38.570149]  batch: 351  d_loss: -4.091559  g_loss: 13.029138\n","Elapsed: [1:27:19.508190]  batch: 364  d_loss: -11.663004  g_loss: 15.806559\n","Elapsed: [1:28:00.351418]  batch: 377  d_loss: -17.697483  g_loss: 18.809364\n","Elapsed: [1:28:41.232753]  batch: 390  d_loss: -8.455147  g_loss: 16.944881\n","Elapsed: [1:29:22.147977]  batch: 403  d_loss: -8.860328  g_loss: 17.707844\n","Elapsed: [1:30:03.047807]  batch: 416  d_loss: -8.257327  g_loss: 13.619181\n","Elapsed: [1:30:43.863384]  batch: 429  d_loss: -0.473542  g_loss: 15.881807\n","Elapsed: [1:31:24.684954]  batch: 442  d_loss: -13.488096  g_loss: 20.848583\n","Elapsed: [1:32:05.569983]  batch: 455  d_loss: -7.882717  g_loss: 10.925897\n","Elapsed: [1:32:46.313080]  batch: 468  d_loss: -4.498150  g_loss: 20.254101\n","Elapsed: [1:33:26.695761]  batch: 481  d_loss: -10.310013  g_loss: 15.961597\n","Elapsed: [1:34:06.935147]  batch: 494  d_loss: -4.645907  g_loss: 22.790405\n","Elapsed: [1:34:47.258721]  batch: 507  d_loss: -11.232130  g_loss: 24.992310\n","Elapsed: [1:35:27.538196]  batch: 520  d_loss: -16.200211  g_loss: 21.638142\n","Elapsed: [1:36:07.686984]  batch: 533  d_loss: -14.445862  g_loss: 31.447121\n","Elapsed: [1:36:47.908363]  batch: 546  d_loss: 5.190504  g_loss: 13.804825\n","Elapsed: [1:37:28.237896]  batch: 559  d_loss: -22.034792  g_loss: 25.486008\n","Elapsed: [1:38:08.543335]  batch: 572  d_loss: 3.019047  g_loss: 8.840095\n","Elapsed: [1:38:48.788235]  batch: 585  d_loss: -15.934593  g_loss: 15.417025\n","Elapsed: [1:39:29.000412]  batch: 598  d_loss: -7.567509  g_loss: 11.444695\n","Elapsed: [1:40:09.283784]  batch: 611  d_loss: -1.901802  g_loss: 5.790936\n","Elapsed: [1:40:49.499175]  batch: 624  d_loss: -15.659982  g_loss: 15.481686\n","Elapsed: [1:41:29.529975]  batch: 637  d_loss: -5.592776  g_loss: 10.049826\n","Elapsed: [1:42:09.626102]  batch: 650  d_loss: -7.191751  g_loss: 12.713317\n","Elapsed: [1:42:49.804186]  batch: 663  d_loss: -4.754296  g_loss: 15.401674\n","Elapsed: [1:43:29.915707]  batch: 676  d_loss: -4.710711  g_loss: 16.736603\n","Elapsed: [1:44:10.196983]  batch: 689  d_loss: -19.454054  g_loss: 15.256778\n","Elapsed: [1:44:51.007594]  batch: 702  d_loss: -6.225539  g_loss: 10.877480\n","Elapsed: [1:45:31.884457]  batch: 715  d_loss: -5.560646  g_loss: 13.409648\n","Elapsed: [1:46:12.338201]  batch: 728  d_loss: -3.201761  g_loss: 15.801345\n","Elapsed: [1:46:52.607739]  batch: 741  d_loss: -1.035362  g_loss: 19.265348\n","Elapsed: [1:47:32.887307]  batch: 754  d_loss: 1.948849  g_loss: 15.998622\n","Elapsed: [1:48:13.267992]  batch: 767  d_loss: -6.255642  g_loss: 24.873257\n","Elapsed: [1:48:53.624220]  batch: 780  d_loss: -11.414004  g_loss: 24.137716\n","Elapsed: [1:49:33.870720]  batch: 793  d_loss: 15.208442  g_loss: 8.606810\n","Elapsed: [1:50:14.107382]  batch: 806  d_loss: -7.430248  g_loss: 11.514306\n","Elapsed: [1:50:54.419379]  batch: 819  d_loss: -20.834412  g_loss: 28.096107\n","Elapsed: [1:51:34.673624]  batch: 832  d_loss: -5.690628  g_loss: 23.346376\n","Elapsed: [1:52:14.887783]  batch: 845  d_loss: 3.330459  g_loss: 12.595548\n","Elapsed: [1:52:55.087305]  batch: 858  d_loss: -4.647430  g_loss: 16.520596\n","Elapsed: [1:53:35.303365]  batch: 871  d_loss: -5.365542  g_loss: 19.682541\n","Elapsed: [1:54:15.469400]  batch: 884  d_loss: -5.453192  g_loss: 10.532236\n","Elapsed: [1:54:55.460136]  batch: 897  d_loss: -1.894119  g_loss: 13.154541\n","Elapsed: [1:55:35.467638]  batch: 910  d_loss: -5.553663  g_loss: 19.749372\n","Elapsed: [1:56:15.568146]  batch: 923  d_loss: -1.813202  g_loss: 12.446719\n","Elapsed: [1:56:55.679420]  batch: 936  d_loss: -14.015781  g_loss: 20.241491\n","Elapsed: [1:57:35.751811]  batch: 949  d_loss: -9.606751  g_loss: 21.597128\n","Elapsed: [1:58:15.636873]  batch: 962  d_loss: -5.425554  g_loss: 20.056948\n","Elapsed: [1:58:55.589725]  batch: 975  d_loss: -1.461284  g_loss: 10.073671\n","Elapsed: [1:59:35.449030]  batch: 988  d_loss: -12.188388  g_loss: 22.782076\n","Elapsed: [2:00:15.247759]  batch: 1001  d_loss: -7.695816  g_loss: 17.293541\n","Elapsed: [2:00:55.000379]  batch: 1014  d_loss: 0.873064  g_loss: 10.746030\n","Elapsed: [2:01:34.917141]  batch: 1027  d_loss: -5.191027  g_loss: 10.556527\n","Elapsed: [2:02:14.773508]  batch: 1040  d_loss: -6.966461  g_loss: 14.555156\n","Elapsed: [2:02:54.577731]  batch: 1053  d_loss: -0.289365  g_loss: 19.991590\n","Elapsed: [2:03:34.428946]  batch: 1066  d_loss: -6.188018  g_loss: 6.890700\n","Elapsed: [2:04:14.286334]  batch: 1079  d_loss: -15.413097  g_loss: 9.537149\n","Elapsed: [2:04:54.142783]  batch: 1092  d_loss: -8.935625  g_loss: 17.723541\n","Elapsed: [2:05:33.927076]  batch: 1105  d_loss: -4.880563  g_loss: 23.434883\n","Elapsed: [2:06:13.631507]  batch: 1118  d_loss: -23.262136  g_loss: 13.969666\n","Elapsed: [2:06:53.468427]  batch: 1131  d_loss: -4.800664  g_loss: 12.462160\n","Elapsed: [2:07:33.255629]  batch: 1144  d_loss: -2.718425  g_loss: 10.361679\n","Elapsed: [2:08:13.039289]  batch: 1157  d_loss: -12.263499  g_loss: 8.292254\n","Elapsed: [2:08:52.803370]  batch: 1170  d_loss: -5.155800  g_loss: 14.172874\n","Elapsed: [2:09:32.666374]  batch: 1183  d_loss: -9.352168  g_loss: 19.598642\n","Elapsed: [2:10:12.484301]  batch: 1196  d_loss: -6.101417  g_loss: 17.813967\n","Elapsed: [2:10:52.225768]  batch: 1209  d_loss: -29.079094  g_loss: 26.351789\n","Elapsed: [2:11:31.986365]  batch: 1222  d_loss: -11.078217  g_loss: 32.948700\n","Elapsed: [2:12:11.802050]  batch: 1235  d_loss: -3.276686  g_loss: 11.074713\n","Elapsed: [2:12:51.623682]  batch: 1248  d_loss: -8.806670  g_loss: 5.320074\n","Elapsed: [2:13:31.388262]  batch: 1261  d_loss: -21.570633  g_loss: 7.063159\n","Elapsed: [2:14:11.076527]  batch: 1274  d_loss: -24.463697  g_loss: 19.012028\n","Elapsed: [2:14:50.873386]  batch: 1287  d_loss: -21.737844  g_loss: 14.614967\n","Elapsed: [2:15:30.605541]  batch: 1300  d_loss: -2.677963  g_loss: 18.128086\n","Time taken for epoch: 4054.534 secs\n","ticker =  70524\n","\n","Epoch: 55\n","Elapsed: [2:15:51.945817]  batch: 1  d_loss: 2.663232  g_loss: 21.565025\n","Elapsed: [2:16:28.653747]  batch: 13  d_loss: -5.411851  g_loss: 22.185711\n","Elapsed: [2:17:08.375629]  batch: 26  d_loss: -16.708502  g_loss: 17.198416\n","Elapsed: [2:17:48.165503]  batch: 39  d_loss: -11.476421  g_loss: 15.455074\n","Elapsed: [2:18:27.901123]  batch: 52  d_loss: -15.185457  g_loss: 29.051600\n","Elapsed: [2:19:07.645538]  batch: 65  d_loss: -15.535728  g_loss: 16.074057\n","Elapsed: [2:19:47.353316]  batch: 78  d_loss: -9.444074  g_loss: 22.721327\n","Elapsed: [2:20:27.223642]  batch: 91  d_loss: -11.814647  g_loss: 24.592690\n","Elapsed: [2:21:07.005675]  batch: 104  d_loss: 0.904300  g_loss: 10.348713\n","Elapsed: [2:21:46.769531]  batch: 117  d_loss: -5.045096  g_loss: 9.080023\n","Elapsed: [2:22:26.560539]  batch: 130  d_loss: -15.119282  g_loss: 17.397545\n","Elapsed: [2:23:06.307743]  batch: 143  d_loss: -10.859824  g_loss: 20.918699\n","Elapsed: [2:23:46.082205]  batch: 156  d_loss: -29.668821  g_loss: 25.357719\n","Elapsed: [2:24:25.786556]  batch: 169  d_loss: -4.864203  g_loss: 22.839148\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-40bf6316bc55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/models/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/samples/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/models/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mstart_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTART_DEPTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTART_EPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     98\u001b[0m     \u001b[0;31m# ======================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-fce8082a5664>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset, epochs, batch_sizes, fade_in_percentage, num_samples, start_depth, start_epoch, num_workers, feedback_factor, log_dir, sample_dir, save_dir, checkpoint_factor)\u001b[0m\n\u001b[1;32m    621\u001b[0m                     \u001b[0;31m# optimize the discriminator:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m                     dis_loss = self.optimize_discriminator(gan_input, images,\n\u001b[0;32m--> 623\u001b[0;31m                                                            current_depth, alpha)\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m                     \u001b[0;31m# optimize the generator:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-fce8082a5664>\u001b[0m in \u001b[0;36moptimize_discriminator\u001b[0;34m(self, noise, real_batch, depth, alpha)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0mfake_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdis_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;31m# optimize discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-d8bfcfcad537>\u001b[0m in \u001b[0;36mdis_loss\u001b[0;34m(self, real_samps, fake_samps, height, alpha)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_gp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;31m# calculate the WGAN-GP (gradient penalty)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mgp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__gradient_penalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_samps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_samps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-d8bfcfcad537>\u001b[0m in \u001b[0;36m__gradient_penalty\u001b[0;34m(self, real_samps, fake_samps, height, alpha, reg_lambda)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# generate random epsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_samps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# create the merge of both real and fake samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"LZyq5O4cqW0p","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1593966902182,"user_tz":-180,"elapsed":8759561,"user":{"displayName":"Manos Panagiotou","photoUrl":"","userId":"06697745822174955863"}}},"source":[""],"execution_count":null,"outputs":[]}]}